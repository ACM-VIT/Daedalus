{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import hashlib\n",
    "import json\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "# Config the matlotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified notMNIST_large.tar.gz\n",
      "Found and verified notMNIST_small.tar.gz\n"
     ]
    }
   ],
   "source": [
    "url = 'http://commondatastorage.googleapis.com/books1000/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if force or not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notMNIST_large already present - Skipping extraction of notMNIST_large.tar.gz.\n",
      "['notMNIST_large\\\\A', 'notMNIST_large\\\\B', 'notMNIST_large\\\\C', 'notMNIST_large\\\\D', 'notMNIST_large\\\\E', 'notMNIST_large\\\\F', 'notMNIST_large\\\\G', 'notMNIST_large\\\\H', 'notMNIST_large\\\\I', 'notMNIST_large\\\\J']\n",
      "notMNIST_small already present - Skipping extraction of notMNIST_small.tar.gz.\n",
      "['notMNIST_small\\\\A', 'notMNIST_small\\\\B', 'notMNIST_small\\\\C', 'notMNIST_small\\\\D', 'notMNIST_small\\\\E', 'notMNIST_small\\\\F', 'notMNIST_small\\\\G', 'notMNIST_small\\\\H', 'notMNIST_small\\\\I', 'notMNIST_small\\\\J']\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "  data_folders = [\n",
    "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, d))]\n",
    "  if len(data_folders) != num_classes:\n",
    "    raise Exception(\n",
    "      'Expected %d folders, one per class. Found %d instead.' % (\n",
    "        num_classes, len(data_folders)))\n",
    "  print(data_folders)\n",
    "  return data_folders\n",
    "  \n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#direct_base =os.getcwd()+\"/notMNIST_large/\"\n",
    "#letters = ['A','B','C','D','E']\n",
    "#import random\n",
    "##   letter_dir = direct_base +i\n",
    "  #  rand_image = random.choice(os.listdir(letter_dir))\n",
    "   # display(Image(filename = letter_dir+'/'+rand_image))\n",
    "   #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notMNIST_large\\A.pickle already present - Skipping pickling.\n",
      "notMNIST_large\\B.pickle already present - Skipping pickling.\n",
      "notMNIST_large\\C.pickle already present - Skipping pickling.\n",
      "notMNIST_large\\D.pickle already present - Skipping pickling.\n",
      "notMNIST_large\\E.pickle already present - Skipping pickling.\n",
      "notMNIST_large\\F.pickle already present - Skipping pickling.\n",
      "notMNIST_large\\G.pickle already present - Skipping pickling.\n",
      "notMNIST_large\\H.pickle already present - Skipping pickling.\n",
      "notMNIST_large\\I.pickle already present - Skipping pickling.\n",
      "notMNIST_large\\J.pickle already present - Skipping pickling.\n",
      "notMNIST_small\\A.pickle already present - Skipping pickling.\n",
      "notMNIST_small\\B.pickle already present - Skipping pickling.\n",
      "notMNIST_small\\C.pickle already present - Skipping pickling.\n",
      "notMNIST_small\\D.pickle already present - Skipping pickling.\n",
      "notMNIST_small\\E.pickle already present - Skipping pickling.\n",
      "notMNIST_small\\F.pickle already present - Skipping pickling.\n",
      "notMNIST_small\\G.pickle already present - Skipping pickling.\n",
      "notMNIST_small\\H.pickle already present - Skipping pickling.\n",
      "notMNIST_small\\I.pickle already present - Skipping pickling.\n",
      "notMNIST_small\\J.pickle already present - Skipping pickling.\n"
     ]
    }
   ],
   "source": [
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "  \"\"\"Load the data for a single letter label.\"\"\"\n",
    "  image_files = os.listdir(folder)\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "  print(folder)\n",
    "  for image_index, image in enumerate(image_files):\n",
    "    image_file = os.path.join(folder, image)\n",
    "    try:\n",
    "      image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "      if image_data.shape != (image_size, image_size):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[image_index, :, :] = image_data\n",
    "    except IOError as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    \n",
    "  num_images = image_index + 1\n",
    "  dataset = dataset[0:num_images, :, :]\n",
    "  if num_images < min_num_images:\n",
    "    raise Exception('Many fewer images than expected: %d < %d' %\n",
    "                    (num_images, min_num_images))\n",
    "    \n",
    "  print('Full dataset tensor:', dataset.shape)\n",
    "  print('Mean:', np.mean(dataset))\n",
    "  print('Standard deviation:', np.std(dataset))\n",
    "  return dataset\n",
    "        \n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "  dataset_names = []\n",
    "  for folder in data_folders:\n",
    "    set_filename = folder + '.pickle'\n",
    "    dataset_names.append(set_filename)\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "      # You may override by setting force=True.\n",
    "      print('%s already present - Skipping pickling.' % set_filename)\n",
    "    else:\n",
    "      print('Pickling %s.' % set_filename)\n",
    "      dataset = load_letter(folder, min_num_images_per_class)\n",
    "      try:\n",
    "        with open(set_filename, 'wb') as f:\n",
    "          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "      except Exception as e:\n",
    "        print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "  return dataset_names\n",
    "\n",
    "train_datasets = maybe_pickle(train_folders, 45000)\n",
    "test_datasets = maybe_pickle(test_folders, 1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x26e81f6cf28>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncorhJ3KK4SdyiuEncorhJ3LqfwHD79ijbD9rsQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26efe1c26a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "folder = pickle.load(open(\"notMNIST_large\\B.pickle\",\"rb\"))\n",
    "img = random.choice(folder)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A train letter 52912\n",
      "A test letter 1873\n",
      "B train letter 52912\n",
      "B test letter 1873\n",
      "C train letter 52912\n",
      "C test letter 1873\n",
      "D train letter 52912\n",
      "D test letter 1873\n",
      "E train letter 52912\n",
      "E test letter 1873\n",
      "F train letter 52912\n",
      "F test letter 1873\n",
      "G train letter 52912\n",
      "G test letter 1872\n",
      "H train letter 52912\n",
      "H test letter 1872\n",
      "I train letter 52912\n",
      "I test letter 1872\n",
      "J train letter 52911\n",
      "J test letter 1872\n"
     ]
    }
   ],
   "source": [
    "letters = [chr(ord('A')+i) for i in range(10)]\n",
    "for i in letters:\n",
    "    train_folder = pickle.load(open('notMNIST_large/'+i +'.pickle','rb'))\n",
    "    print(i+\" train letter\",len(train_folder))\n",
    "    test_folder = pickle.load(open('notMNIST_small/'+i+'.pickle','rb'))\n",
    "    print(i+' test letter',len(test_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (200000, 28, 28) (200000,)\n",
      "Validation: (10000, 28, 28) (10000,)\n",
      "Testing: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "def make_arrays(nb_rows, img_size):\n",
    "  if nb_rows:\n",
    "    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "    labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "  else:\n",
    "    dataset, labels = None, None\n",
    "  return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "  num_classes = len(pickle_files)\n",
    "  valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "  train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "  vsize_per_class = valid_size // num_classes\n",
    "  tsize_per_class = train_size // num_classes\n",
    "    \n",
    "  start_v, start_t = 0, 0\n",
    "  end_v, end_t = vsize_per_class, tsize_per_class\n",
    "  end_l = vsize_per_class+tsize_per_class\n",
    "  for label, pickle_file in enumerate(pickle_files):       \n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        letter_set = pickle.load(f)\n",
    "        # let's shuffle the letters to have random validation and training set\n",
    "        np.random.shuffle(letter_set)\n",
    "        if valid_dataset is not None:\n",
    "          valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "          valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "          valid_labels[start_v:end_v] = label\n",
    "          start_v += vsize_per_class\n",
    "          end_v += vsize_per_class\n",
    "                    \n",
    "        train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "        train_dataset[start_t:end_t, :, :] = train_letter\n",
    "        train_labels[start_t:end_t] = label\n",
    "        start_t += tsize_per_class\n",
    "        end_t += tsize_per_class\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "      raise\n",
    "    \n",
    "  return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "train_size = 200000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'valid_dataset': valid_dataset,\n",
    "    'valid_labels': valid_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 690800506\n"
     ]
    }
   ],
   "source": [
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28)\n",
      "155\n",
      "3624\n",
      "203\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "all_data = pickle.load(open('notMNIST.pickle','rb'))\n",
    "print(all_data['test_dataset'].shape)\n",
    "def common_elements(dataset1,dataset2):\n",
    "    dup_indices = []\n",
    "    hash_table = [hashlib.sha1(x).hexdigest() for x in dataset1]\n",
    "    for i in range(len(dataset2)):\n",
    "        if (hashlib.sha1(dataset2[i]).hexdigest() in hash_table):\n",
    "            dup_indices.append(i)\n",
    "    return(len(dup_indices))\n",
    "print(common_elements(all_data['test_dataset'], all_data['valid_dataset']))\n",
    "print(common_elements(all_data['test_dataset'], all_data['train_dataset']))\n",
    "print(common_elements(all_data['valid_dataset'], all_data['test_dataset']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = all_data['train_dataset']\n",
    "train_labels = all_data['train_labels']\n",
    "test_dataset = all_data['test_dataset']\n",
    "test_labels = all_data['test_labels']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 trainsamples score: 0.6999\n",
      "1000 trainsamples score: 0.8332\n",
      "5000 trainsamples score: 0.8517\n"
     ]
    }
   ],
   "source": [
    "def get_score(train_dataset, train_labels, test_dataset, test_labels):\n",
    "    model = LogisticRegression()\n",
    "    train_flatten_dataset = np.array([x.flatten() for x in train_dataset])\n",
    "    test_flatten_dataset = np.array([x.flatten() for x in test_dataset])\n",
    "    model.fit(train_flatten_dataset, train_labels)\n",
    "\n",
    "    return model.score(test_flatten_dataset, test_labels)\n",
    "\n",
    "print(\"100 trainsamples score: \" + str(get_score(train_dataset[:100], train_labels[:100], test_dataset, test_labels)))\n",
    "print(\"1000 trainsamples score: \" + str(get_score(train_dataset[:1000], train_labels[:1000], test_dataset, test_labels)))\n",
    "print(\"5000 trainsamples score: \" + str(get_score(train_dataset[:5000], train_labels[:5000], test_dataset, test_labels)))\n",
    "print(\"10000 trainsamples score: \" + str(get_score(train_dataset[:10000], train_labels[:10000], test_dataset, test_labels)))\n",
    "print(\"all trainsamples score: \" + str(get_score(train_dataset, train_labels, test_dataset, test_labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
